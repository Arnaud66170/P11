# src/utils.py

import os
import shutil
import warnings
from pyspark.sql import DataFrame
import tensorflow as tf
import gc

# Suppression des avertissements
warnings.filterwarnings('ignore')

def export_dataframe_if_needed(df, output_path, force_overwrite=True):
    """
    Cette fonction sauvegarde un DataFrame Spark au format Parquet avec gestion intelligente
    des r√©pertoires existants et des erreurs.
    
    Param√®tres :
    - df : DataFrame Spark √† sauvegarder
    - output_path : chemin de sauvegarde (format .parquet)
    - force_overwrite : si True, √©crase le fichier existant
    
    Retourne :
    - True si la sauvegarde a r√©ussi, False sinon
    """
    try:
        # Cr√©ation du r√©pertoire parent si n√©cessaire
        parent_dir = os.path.dirname(output_path)
        if parent_dir and not os.path.exists(parent_dir):
            os.makedirs(parent_dir, exist_ok=True)
            print(f"üìÅ R√©pertoire cr√©√© : {parent_dir}")
        
        # Suppression du r√©pertoire existant si force_overwrite
        if os.path.exists(output_path) and force_overwrite:
            shutil.rmtree(output_path)
        
        # Sauvegarde du DataFrame
        df.write.mode("overwrite" if force_overwrite else "error").parquet(output_path)
        
        # V√©rification de la taille du fichier
        if os.path.exists(output_path):
            size_mb = get_directory_size(output_path) / (1024 * 1024)
            print(f"üíæ Sauvegarde r√©ussie : {output_path} ({size_mb:.1f} MB)")
            return True
        else:
            return False
            
    except Exception as e:
        return False

def get_directory_size(directory_path):
    """
    Calcule la taille totale d'un r√©pertoire en bytes.
    Utile pour les fichiers Parquet qui sont des r√©pertoires.
    
    Param√®tres :
    - directory_path : chemin du r√©pertoire
    
    Retourne :
    - taille en bytes
    """
    total_size = 0
    try:
        for dirpath, dirnames, filenames in os.walk(directory_path):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                if os.path.exists(filepath):
                    total_size += os.path.getsize(filepath)
    except Exception:
        pass  # Ignore silencieusement les erreurs
    
    return total_size

def clean_cache_directory(cache_path, confirm=False):
    """
    Cette fonction nettoie le r√©pertoire de cache pour forcer les recalculs.
    Utile lors du d√©bogage ou pour des tests propres.
    
    Param√®tres :
    - cache_path : chemin du r√©pertoire de cache √† nettoyer
    - confirm : si True, demande confirmation avant suppression
    """
    if not os.path.exists(cache_path):
        return
    
    if confirm:
        response = input(f"üóëÔ∏è  Supprimer le cache {cache_path} ? (o/n): ")
        if response.lower() != 'o':
            return
    
    try:
        shutil.rmtree(cache_path)
        print(f"‚úÖ Cache supprim√© : {cache_path}")
    except Exception:
        pass  # Ignore silencieusement les erreurs

def validate_spark_dataframe(df, expected_columns=None, min_rows=1):
    """
    Cette fonction valide la structure et le contenu d'un DataFrame Spark.
    Utile pour s'assurer de la coh√©rence des donn√©es entre les √©tapes.
    
    Param√®tres :
    - df : DataFrame Spark √† valider
    - expected_columns : liste des colonnes attendues (optionnel)
    - min_rows : nombre minimum de lignes attendu
    
    Retourne :
    - True si toutes les validations passent, False sinon
    """
    try:
        # V√©rification de l'existence du DataFrame
        if df is None:
            return False
        
        # V√©rification du nombre de lignes
        row_count = df.count()
        if row_count < min_rows:
            return False
        
        # V√©rification des colonnes attendues
        if expected_columns:
            actual_columns = set(df.columns)
            expected_set = set(expected_columns)
            
            missing_columns = expected_set - actual_columns
            if missing_columns:
                return False
        
        print(f"‚úÖ DataFrame valide : {row_count} lignes, {len(df.columns)} colonnes")
        return True
        
    except Exception:
        return False

def print_dataframe_summary(df, title="R√©sum√© du DataFrame", show_sample=True, sample_size=5):
    """
    Cette fonction affiche un r√©sum√© d√©taill√© d'un DataFrame Spark.
    Tr√®s utile pour le d√©bogage et la validation des donn√©es.
    
    Param√®tres :
    - df : DataFrame Spark √† analyser
    - title : titre √† afficher
    - show_sample : si True, affiche un √©chantillon des donn√©es
    - sample_size : nombre de lignes √† afficher dans l'√©chantillon
    """
    print(f"\n{'='*60}")
    print(f"üìä {title.upper()}")
    print(f"{'='*60}")
    
    try:
        # Informations g√©n√©rales
        row_count = df.count()
        col_count = len(df.columns)
        
        print(f"üìà Nombre de lignes : {row_count:,}")
        print(f"üìã Nombre de colonnes : {col_count}")
        
        # Sch√©ma du DataFrame
        print(f"\nüóÇÔ∏è  Sch√©ma des donn√©es :")
        df.printSchema()
        
        # √âchantillon des donn√©es
        if show_sample and row_count > 0:
            print(f"\nüîç √âchantillon des donn√©es (premi√®re {min(sample_size, row_count)} lignes) :")
            df.show(sample_size, truncate=False)
        
        # Statistiques sur les colonnes nulles
        print(f"\nüìä Statistiques des valeurs nulles :")
        for col_name in df.columns:
            null_count = df.filter(df[col_name].isNull()).count()
            if null_count > 0:
                null_percentage = (null_count / row_count) * 100 if row_count > 0 else 0
                print(f"   ‚ö†Ô∏è  {col_name}: {null_count:,} nulls ({null_percentage:.1f}%)")
            else:
                print(f"   ‚úÖ {col_name}: aucune valeur nulle")
                
    except Exception:
        pass  # Ignore silencieusement les erreurs
    
    print(f"{'='*60}\n")

def setup_project_directories(base_path="../"):
    """
    Cette fonction cr√©e l'arborescence compl√®te du projet si elle n'existe pas.
    Garantit que tous les r√©pertoires n√©cessaires sont pr√©sents.
    
    Param√®tres :
    - base_path : chemin de base du projet
    
    Retourne :
    - dictionnaire contenant tous les chemins cr√©√©s
    """
    directories = {
        "data": os.path.join(base_path, "data"),
        "outputs": os.path.join(base_path, "outputs"), 
        "cache": os.path.join(base_path, "outputs", "cache"),
        "results_local": os.path.join(base_path, "outputs", "Results_Local"),
        "results_cloud": os.path.join(base_path, "outputs", "Results_Cloud"),
        "logs": os.path.join(base_path, "outputs", "logs"),
        "plots": os.path.join(base_path, "outputs", "plots")
    }
    
    created_dirs = []
    
    for name, path in directories.items():
        if not os.path.exists(path):
            os.makedirs(path, exist_ok=True)
            created_dirs.append(path)
    
    if created_dirs:
        print(f"üìÅ R√©pertoires cr√©√©s : {len(created_dirs)}")
        for dir_path in created_dirs:
            print(f"   - {dir_path}")
    else:
        print("üìÅ Tous les r√©pertoires existent d√©j√†")
    
    return directories

def log_processing_step(step_name, details="", log_path="../outputs/logs/processing.log"):
    """
    Cette fonction enregistre les √©tapes de traitement dans un fichier de log.
    Utile pour tracer l'ex√©cution et d√©boguer les probl√®mes.
    
    Param√®tres :
    - step_name : nom de l'√©tape
    - details : d√©tails suppl√©mentaires (optionnel)
    - log_path : chemin du fichier de log
    """
    import datetime
    
    # Cr√©ation du r√©pertoire de log si n√©cessaire
    os.makedirs(os.path.dirname(log_path), exist_ok=True)
    
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_entry = f"[{timestamp}] {step_name}"
    
    if details:
        log_entry += f" - {details}"
    
    try:
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(log_entry + "\n")
        
        print(f"üìù Log enregistr√© : {step_name}")
    except Exception:
        pass  # Ignore silencieusement les erreurs

def check_system_resources():
    """
    Cette fonction v√©rifie les ressources syst√®me disponibles.
    Utile pour optimiser les param√®tres Spark selon l'environnement.
    
    Retourne :
    - dictionnaire contenant les informations syst√®me
    """
    try:
        import psutil
        
        resources = {
            "cpu_cores": psutil.cpu_count(logical=False),
            "cpu_threads": psutil.cpu_count(logical=True),
            "memory_gb": psutil.virtual_memory().total / (1024**3),
            "memory_available_gb": psutil.virtual_memory().available / (1024**3),
            "disk_free_gb": psutil.disk_usage('/').free / (1024**3)
        }
        
        print("üñ•Ô∏è  Ressources syst√®me d√©tect√©es :")
        print(f"   - CPU : {resources['cpu_cores']} c≈ìurs, {resources['cpu_threads']} threads")
        print(f"   - RAM : {resources['memory_gb']:.1f} GB total, {resources['memory_available_gb']:.1f} GB disponible")
        print(f"   - Disque : {resources['disk_free_gb']:.1f} GB libres")
        
        # Recommandations Spark
        recommended_executor_cores = min(4, resources['cpu_cores'])
        recommended_executor_memory = int(resources['memory_available_gb'] * 0.6)
        
        print(f"\nüí° Recommandations Spark :")
        print(f"   - spark.executor.cores: {recommended_executor_cores}")
        print(f"   - spark.executor.memory: {recommended_executor_memory}g")
        
        return resources
        
    except ImportError:
        print("‚ö†Ô∏è  psutil non disponible - informations syst√®me limit√©es")
        return {}

def clean_gpu_cache(verbose: bool = True):
    """
    Lib√®re la m√©moire GPU et nettoie les objets inutiles de TensorFlow/Keras.
    Utile pour √©viter les fuites m√©moire en environnement GPU.
    """
    try:
        # Suppression explicite des graphes et objets
        tf.keras.backend.clear_session()
        gc.collect()

        # R√©initialisation des allocations m√©moire GPU (si applicable)
        gpus = tf.config.list_physical_devices('GPU')
        if gpus and verbose:
            print("[INFO] M√©moire GPU nettoy√©e avec succ√®s.")

    except Exception as e:
        print(f"[ERREUR] Impossible de nettoyer la m√©moire GPU : {e}")