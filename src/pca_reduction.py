# src/pca_reduction.py

import os
import warnings
import numpy as np
import matplotlib.pyplot as plt

# Suppression des avertissements
warnings.filterwarnings('ignore')
plt.rcParams.update({'figure.max_open_warning': 0})

from pyspark.ml import PipelineModel
from pyspark.ml.feature import PCA, VectorAssembler
from pyspark.ml.linalg import Vectors, DenseVector, VectorUDT
# Import sp√©cifique pour √©viter les conflits de noms avec les fonctions Python
from pyspark.sql import functions as F
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, IntegerType
import shutil

def convert_array_to_vector(df, features_col="features"):
    """
    Cette fonction convertit une colonne d'arrays Python/NumPy en vecteurs Spark ML.
    N√©cessaire car MobileNetV2 produit des arrays Python, mais PCA Spark attend des VectorUDT.
    
    Param√®tres :
    - df : DataFrame Spark avec une colonne 'features' au format array
    - features_col : nom de la colonne √† convertir
    
    Retourne :
    - DataFrame avec la colonne convertie en format VectorUDT
    """
    print("üîÑ Conversion des arrays Python vers des vecteurs Spark ML...")
    
    # UDF pour convertir un array en DenseVector Spark ML
    def array_to_vector(arr):
        if arr is None:
            return None
        # Convertit l'array en DenseVector Spark ML
        return Vectors.dense(arr)
    
    # Enregistrement de l'UDF avec le bon type de retour
    array_to_vector_udf = udf(array_to_vector, VectorUDT())
    
    # Application de la conversion
    df_converted = df.withColumn(features_col + "_vector", array_to_vector_udf(col(features_col)))
    df_converted = df_converted.drop(features_col).withColumnRenamed(features_col + "_vector", features_col)
    
    print("‚úÖ Conversion termin√©e - les features sont maintenant au format VectorUDT")
    return df_converted

def get_optimal_pca_k(df, spark, max_k=200, threshold=0.95, force_retrain=False, cache_path="../outputs/pca_variance.parquet"):
    """
    D√©termine le nombre optimal de composantes principales √† conserver en fonction du seuil de variance cumul√©e souhait√©e.
    
    ‚ö†Ô∏è CORRECTION MAJEURE : Cette fonction calcule correctement la variance cumul√©e
    en stockant TOUTES les variances individuelles, pas seulement la derni√®re valeur.

    Param√®tres :
    - df : DataFrame Spark contenant les features √† r√©duire (format VectorUDT)
    - spark : SparkSession active
    - max_k : nombre maximal de composantes √† tester (augment√© √† 200 pour MobileNetV2)
    - threshold : seuil de variance expliqu√©e cumul√©e √† atteindre (ex : 0.95)
    - force_retrain : recalcul m√™me si un cache existe
    - cache_path : chemin de sauvegarde des variances cumul√©es

    Retourne :
    - k_optimal : int, nombre de composantes √† retenir
    - variance_data : liste des tuples (k, variance_individuelle, variance_cumul√©e)
    """
    from pyspark.ml.feature import PCA
    from pyspark.ml.linalg import Vectors, VectorUDT
    import os

    # V√©rifie que la colonne 'features' est bien un vecteur Spark ML
    if not isinstance(df.schema["features"].dataType, VectorUDT):
        print("‚ùå Erreur : La colonne 'features' doit √™tre de type VectorUDT (Spark ML).")
        print("üí° Conseil : Utilise d'abord convert_array_to_vector() pour convertir tes donn√©es.")
        raise TypeError("La colonne 'features' doit √™tre de type VectorUDT (Spark ML).")

    if os.path.exists(cache_path) and not force_retrain:
        print(f"‚úÖ Cache de variance charg√© depuis {cache_path}")
        df_variance = spark.read.parquet(cache_path)
        
        # Reconstruction des donn√©es pour le retour
        variance_data = []
        rows = df_variance.orderBy("k").collect()
        for row in rows:
            variance_data.append((row["k"], row["individual_variance"], row["cum_variance"]))
            
    else:
        print("üÜï Pas de cache d√©tect√© ou recalcul forc√© : calcul des variances cumul√©es...")
        print(f"üìä Test de k=1 √† k={max_k} composantes principales...")
        
        variance_data = []
        cumulative_variance = 0.0
        
        for k in range(1, max_k + 1):
            print(f"   Calcul PCA pour k={k}...", end=" ")
            
            # Cr√©ation et entra√Ænement du mod√®le PCA
            pca = PCA(k=k, inputCol="features", outputCol="pca_features")
            model = pca.fit(df)
            
            # R√©cup√©ration des variances expliqu√©es individuelles
            explained_variances = model.explainedVariance.toArray()
            
            # ‚úÖ CORRECTION : Calcul correct de la variance cumul√©e
            # On somme TOUTES les variances individuelles jusqu'√† k
            cumulative_variance = float(np.sum(explained_variances))
            
            # Variance individuelle de la k-i√®me composante
            individual_variance = float(explained_variances[-1])
            
            variance_data.append((k, individual_variance, cumulative_variance))
            
            print(f"Variance cumul√©e: {cumulative_variance:.4f} ({cumulative_variance*100:.2f}%)")
            
            # Arr√™t anticip√© si le seuil est atteint
            if cumulative_variance >= threshold:
                print(f"üéØ Seuil de {threshold*100:.1f}% atteint avec k={k}")
                break

        # Sauvegarde du cache
        schema = StructType([
            StructField("k", IntegerType(), True),
            StructField("individual_variance", DoubleType(), True),
            StructField("cum_variance", DoubleType(), True)
        ])
        
        df_variance = spark.createDataFrame(variance_data, schema)
        df_variance.write.mode("overwrite").parquet(cache_path)
        print(f"üíæ Variance cumul√©e sauvegard√©e dans {cache_path}")

    # Affichage des r√©sultats
    print("\nüß™ Aper√ßu des r√©sultats :")
    df_variance.orderBy("k").show(min(20, max_k), truncate=False)
    
    # Recherche du k optimal
    optimal_rows = df_variance.filter(df_variance["cum_variance"] >= threshold).orderBy("k")

    if optimal_rows.count() == 0:
        print(f"‚ö†Ô∏è  Aucune valeur de k (jusqu'√† {max_k}) ne permet d'atteindre {threshold*100:.1f}% de variance.")
        k_optimal = max_k
        actual_variance = df_variance.orderBy(F.col("k").desc()).first()["cum_variance"]
        print(f"üìä Avec k={k_optimal}, variance expliqu√©e = {actual_variance:.4f} ({actual_variance*100:.2f}%)")
        print(f"üí° Conseil : Augmente max_k ou diminue le threshold.")
    else:
        row = optimal_rows.first()
        k_optimal = int(row["k"])
        actual_variance = row["cum_variance"]
        print(f"‚úÖ {k_optimal} composantes n√©cessaires pour expliquer au moins {threshold*100:.1f}% de la variance.")
        print(f"üìä Variance exacte atteinte avec k={k_optimal} : {actual_variance:.4f} ({actual_variance*100:.2f}%)")

    return k_optimal, variance_data

def plot_variance_explained(variance_data, threshold=0.95, save_path="../outputs/pca_variance_plot.png"):
    """
    Cette fonction g√©n√®re un graphique de la variance expliqu√©e cumul√©e
    pour visualiser l'√©volution et identifier le coude optimal.
    
    Param√®tres :
    - variance_data : liste des tuples (k, variance_individuelle, variance_cumul√©e)
    - threshold : seuil √† afficher sur le graphique
    - save_path : chemin de sauvegarde du graphique
    """
    print("üìà G√©n√©ration du graphique de variance expliqu√©e...")
    
    # Extraction des donn√©es
    k_values = [item[0] for item in variance_data]
    individual_variances = [item[1] for item in variance_data]
    cumulative_variances = [item[2] for item in variance_data]
    
    # Cr√©ation du graphique sans avertissements
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
        
        # Graphique 1 : Variance cumul√©e
        ax1.plot(k_values, cumulative_variances, 'b-', linewidth=2, marker='o', markersize=4)
        ax1.axhline(y=threshold, color='r', linestyle='--', linewidth=2, label=f'Seuil {threshold*100:.0f}%')
        ax1.set_xlabel('Nombre de composantes principales (k)')
        ax1.set_ylabel('Variance expliqu√©e cumul√©e')
        ax1.set_title('√âvolution de la variance expliqu√©e cumul√©e en fonction de k')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        ax1.set_ylim(0, 1)
        
        # Graphique 2 : Variance individuelle par composante
        ax2.bar(k_values, individual_variances, alpha=0.7, color='green')
        ax2.set_xlabel('Num√©ro de la composante principale')
        ax2.set_ylabel('Variance expliqu√©e individuelle')
        ax2.set_title('Contribution individuelle de chaque composante principale')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # Sauvegarde
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()  # Ferme la figure pour √©viter les avertissements
    
    print(f"üíæ Graphique sauvegard√© : {save_path}")

def apply_pca_on_features(spark, df, k=100, features_col="features", output_path="../outputs/Results_Local/features_pca.parquet", force_retrain=False):
    """
    Applique une r√©duction PCA sur un DataFrame Spark.
    
    ‚ö†Ô∏è CORRECTION : Cette fonction v√©rifie maintenant le format des donn√©es d'entr√©e

    Param√®tres :
    - spark : SparkSession active √† utiliser pour les lectures/√©critures
    - df : DataFrame Spark contenant les vecteurs (format VectorUDT)
    - k : int, nombre de composantes principales √† conserver
    - features_col : str, colonne contenant les vecteurs d'entr√©e
    - output_path : str, chemin du .parquet r√©sultant
    - force_retrain : bool, si False, recharge depuis cache si existant

    Retourne :
    - DataFrame Spark avec les features r√©duites
    """
    import os
    from pyspark.ml.feature import PCA
    # Import local pour √©viter les probl√®mes de chemin
    from utils import export_dataframe_if_needed

    # V√©rification du format des donn√©es
    if not isinstance(df.schema[features_col].dataType, VectorUDT):
        print("‚ùå Erreur : La colonne features doit √™tre de type VectorUDT")
        print("üí° Conseil : Utilise convert_array_to_vector() avant d'appeler cette fonction")
        raise TypeError(f"La colonne '{features_col}' doit √™tre de type VectorUDT (Spark ML).")

    if os.path.exists(output_path) and not force_retrain:
        print(f"‚úî Chargement des donn√©es PCA depuis le cache : {output_path}")
        return spark.read.parquet(output_path)

    print(f"‚öô Calcul PCA en cours avec k={k} composantes...")
    
    # Cr√©ation et entra√Ænement du mod√®le PCA
    pca = PCA(k=k, inputCol=features_col, outputCol="features_pca")
    model = pca.fit(df)
    
    # Application de la transformation
    df_pca = model.transform(df)
    
    # Ajout de m√©tadonn√©es pour tra√ßabilit√©
    print(f"üìä R√©duction de dimensionnalit√© appliqu√©e :")
    print(f"   - Dimensions originales : probablement 1280 (MobileNetV2)")
    print(f"   - Dimensions r√©duites : {k}")
    print(f"   - Facteur de r√©duction : ~{1280/k:.1f}x")
    
    # Sauvegarde
    export_dataframe_if_needed(df_pca, output_path)

    return df_pca

def plot_variance_curve(variance_data, k_optimal, threshold=0.95, save_path=None):
    """
    Affiche la courbe de variance cumul√©e avec annotations propres.
    
    Cette fonction g√®re  tous les cas de figure
    
    :param variance_data: liste de tuples (k, var_individuelle, var_cumul√©e)
    :param k_optimal: entier, nombre de composantes optimales
    :param threshold: seuil de variance cumul√©e √† atteindre (ex: 0.95)
    :param save_path: chemin d'enregistrement (facultatif)
    """
    if not variance_data:
        print("‚ùå Erreur : Aucune donn√©e de variance fournie")
        return
    
    ks = [int(row[0]) for row in variance_data]
    cum_vars = [row[2] for row in variance_data]

    # ‚úÖ CORRECTION : V√©rifications approfondies
    print(f"üîç Debug - k_optimal: {k_optimal}")
    print(f"üìä Donn√©es disponibles: k de {min(ks)} √† {max(ks)} ({len(ks)} points)")
    
    # Cr√©er un dictionnaire pour un acc√®s direct k -> variance cumul√©e
    k_to_variance = dict(zip(ks, cum_vars))
    
    # ‚úÖ CORRECTION : Gestion intelligente du k_optimal non trouv√©
    if k_optimal not in k_to_variance:
        print(f"‚ö†Ô∏è k_optimal={k_optimal} n'est pas dans les donn√©es calcul√©es!")
        
        # Strat√©gie 1 : Chercher le k le plus proche qui atteint le threshold
        valid_ks_above_threshold = [k for k, var in k_to_variance.items() if var >= threshold]
        
        if valid_ks_above_threshold:
            k_optimal_adjusted = min(valid_ks_above_threshold)
            print(f"üîß Utilisation de k={k_optimal_adjusted} (plus petit k atteignant {threshold*100}%)")
        else:
            # Strat√©gie 2 : Prendre le k maximum disponible
            k_optimal_adjusted = max(ks)
            print(f"üîß Fallback sur k={k_optimal_adjusted} (k maximum disponible)")
            print(f"‚ö†Ô∏è Ce k ne permet d'atteindre que {k_to_variance[k_optimal_adjusted]*100:.2f}% de variance")
        
        k_optimal = k_optimal_adjusted
    
    # R√©cup√©rer la variance correspondante
    optimal_variance = k_to_variance[k_optimal]
    print(f"‚úÖ Point optimal utilis√©: k={k_optimal}, variance={optimal_variance:.4f} ({optimal_variance*100:.2f}%)")
    
    # ‚úÖ G√©n√©ration du graphique robuste
    plt.figure(figsize=(10, 6))
    plt.plot(ks, cum_vars, marker='o', linestyle='-', color='blue', label='Variance expliqu√©e cumul√©e')
    plt.axhline(y=threshold, color='red', linestyle='--', label=f'Seuil {int(threshold*100)}%')
    plt.axvline(x=k_optimal, color='green', linestyle='--', label=f'{k_optimal} composantes')
    
    # Point optimal mis en evidence
    plt.scatter(k_optimal, optimal_variance, color='black', s=100, zorder=5)
    plt.text(k_optimal, optimal_variance + 0.01, f'{optimal_variance*100:.2f}%', 
             ha='center', fontsize=9, bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))

    plt.xlabel("Nombre de composantes")
    plt.ylabel("Variance expliqu√©e cumul√©e")
    plt.title("Variance expliqu√©e cumul√©e par la PCA (Spark)")
    plt.grid(True, alpha=0.3)
    
    # Am√©lioration des axes
    plt.xticks(ks[::max(1, len(ks)//20)])  # espacement dynamique
    plt.ylim(0, 1.05)  # Un peu d'espace au-dessus
    plt.legend()
    plt.tight_layout()

    if save_path:
        import os
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"üíæ Graphique sauvegard√© : {save_path}")
    else:
        plt.show()
    
    plt.close()  # ‚úÖ Fermeture explicite pour √©viter les fuites m√©moire
