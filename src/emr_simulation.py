# src/emr_simulation.py
"""
Module de simulation EMR/S3 pour le projet P11 - Version optimis√©e
Permet de basculer entre local, simulation LocalStack et production AWS
avec gestion intelligente de la m√©moire et support GPU
"""

import os
import boto3
from pyspark.sql import SparkSession
from pathlib import Path

class EMRSimulation:
    """
    Classe principale pour g√©rer les diff√©rents modes d'ex√©cution :
    - local : Filesystem local + Spark local
    - simulation : LocalStack S3 + Spark local  
    - production : AWS S3 + EMR
    """
    
    def __init__(self, mode="local", enable_gpu=False):
        """
        Initialise la configuration selon le mode choisi
        
        Args:
            mode (str): "local", "simulation" ou "production"
            enable_gpu (bool): Active les optimisations GPU pour mode local
        """
        self.mode = mode
        self.enable_gpu = enable_gpu
        self.config = self._get_config()
        print(f"üîß EMRSimulation initialis√© en mode: {mode}")
        if enable_gpu and mode == "local":
            print("üöÄ Optimisations GPU activ√©es")
    
    def _get_config(self):
        """Retourne la configuration selon le mode"""
        if self.mode == "local":
            return {
                "spark_master": "local[*]",
                "storage_type": "filesystem",
                "base_path": "./outputs",
                "s3_endpoint": None
            }
        elif self.mode == "simulation":
            return {
                "spark_master": "local[*]",
                "storage_type": "s3_local",
                "base_path": "s3a://fruits-p11-simulation",
                "s3_endpoint": "http://localhost:4566"
            }
        elif self.mode == "production":
            return {
                "spark_master": None,  # G√©r√© par EMR
                "storage_type": "s3_aws",
                "base_path": "s3a://fruits-p11-production",
                "s3_endpoint": None
            }
        else:
            raise ValueError(f"Mode non support√©: {self.mode}")
    
    def get_memory_config(self):
        """
        Retourne la configuration m√©moire optimis√©e selon le mode et GPU
        """
        if self.mode == "local":
            if self.enable_gpu:
                # GPU local optimis√© (GTX 1060 6GB)
                return {
                    "spark.executor.memory": "6g",
                    "spark.driver.memory": "4g",
                    "spark.executor.memoryFraction": "0.8"
                }
            else:
                # CPU local standard
                return {
                    "spark.executor.memory": "4g",
                    "spark.driver.memory": "2g"
                }
                
        elif self.mode == "simulation":
            # LocalStack optimis√© (moins de RAM n√©cessaire)
            return {
                "spark.executor.memory": "3g",
                "spark.driver.memory": "2g"
            }
            
        elif self.mode == "production":
            # AWS EMR optimis√© (instances m5.xlarge = 16GB RAM)
            return {
                "spark.executor.memory": "7g",
                "spark.driver.memory": "4g",
                "spark.executor.memoryFraction": "0.8"
            }
    
    def get_spark_config(self):
        """Retourne la configuration Spark compl√®te adapt√©e au mode"""
        # Configuration de base commune
        base_config = {
            "spark.sql.adaptive.enabled": "true",
            "spark.sql.adaptive.coalescePartitions.enabled": "true",
            "spark.serializer": "org.apache.spark.serializer.KryoSerializer"
        }
        
        # Ajout configuration m√©moire
        memory_config = self.get_memory_config()
        base_config.update(memory_config)
        
        # Optimisations GPU et Arrow
        if self.mode == "local" and self.enable_gpu:
            base_config.update({
                "spark.sql.execution.arrow.pyspark.enabled": "true"
            })
        elif self.mode == "production":
            base_config.update({
                "spark.sql.execution.arrow.pyspark.enabled": "true"
            })
        
        # Configuration S3 selon le mode
        if self.mode == "simulation":
            # Configuration pour LocalStack S3
            base_config.update({
                "spark.hadoop.fs.s3a.endpoint": "http://localhost:4566",
                "spark.hadoop.fs.s3a.access.key": "test",
                "spark.hadoop.fs.s3a.secret.key": "test",
                "spark.hadoop.fs.s3a.path.style.access": "true",
                "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem",
                "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider",
                "spark.sql.parquet.writeLegacyFormat": "true"
            })
        elif self.mode == "production":
            # Configuration pour AWS S3 (utilise les credentials IAM)
            base_config.update({
                "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem",
                "spark.hadoop.fs.s3a.aws.credentials.provider": "com.amazonaws.auth.InstanceProfileCredentialsProvider",
                "spark.sql.parquet.writeLegacyFormat": "true"
            })
        
        return base_config
    
    def get_storage_path(self, subpath=""):
        """Retourne le chemin de stockage selon le mode"""
        if self.mode == "local":
            base = Path(self.config["base_path"])
            base.mkdir(parents=True, exist_ok=True)
            if subpath:
                full_path = base / subpath
                full_path.parent.mkdir(parents=True, exist_ok=True)
                return str(full_path)
            return str(base)
        else:
            # Modes S3 (simulation ou production)
            if subpath:
                return f"{self.config['base_path']}/{subpath}"
            return self.config["base_path"]
    
    def get_s3_config(self):
        """Retourne la configuration S3 selon le mode"""
        if self.mode == "local":
            return None
        elif self.mode == "simulation":
            return {
                "endpoint_url": "http://localhost:4566",
                "aws_access_key_id": "test",
                "aws_secret_access_key": "test",
                "region_name": "us-east-1"
            }
        else:  # production
            return {
                "region_name": "eu-west-1"  # Conformit√© RGPD
            }
    
    def test_s3_connection(self):
        """Test rapide de connexion S3"""
        if self.mode == "local":
            return True
        
        try:
            s3_client = get_s3_client(mode=self.mode)
            if s3_client:
                # Test simple : lister les buckets
                s3_client.list_buckets()
                return True
        except Exception as e:
            print(f"‚ùå Test S3 √©chou√©: {e}")
            return False

def get_spark_session(mode="local", app_name="FruitsPipeline", enable_gpu=False):
    """
    Cr√©e une session Spark adapt√©e au mode d'ex√©cution SANS conflit de configuration
    
    Args:
        mode (str): "local", "simulation" ou "production"
        app_name (str): Nom de l'application Spark
        enable_gpu (bool): Active les optimisations GPU locales
    
    Returns:
        SparkSession: Session Spark configur√©e
    """
    # D√©tection automatique GPU si pas sp√©cifi√© explicitement
    if mode == "local" and not enable_gpu:
        try:
            import tensorflow as tf
            gpus = tf.config.list_physical_devices('GPU')
            enable_gpu = len(gpus) > 0
            if enable_gpu:
                print("üöÄ GPU d√©tect√© automatiquement")
        except:
            pass
    
    emr_sim = EMRSimulation(mode=mode, enable_gpu=enable_gpu)
    
    # ‚úÖ CONFIGURATION UNIFI√âE (pas de conflit)
    final_config = emr_sim.get_spark_config()
    
    # Construction de la session Spark
    builder = SparkSession.builder.appName(app_name)
    
    # Ajout des packages Hadoop n√©cessaires pour S3A
    builder = builder.config("spark.jars.packages", 
                         "org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.696")

    # Configuration S3A sp√©cifique pour mode simulation ou production
    if mode in ["simulation", "production"]:
        builder = builder \
            .config("spark.hadoop.fs.s3a.access.key", "dummy") \
            .config("spark.hadoop.fs.s3a.secret.key", "dummy") \
            .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:4566") \
            .config("spark.hadoop.fs.s3a.path.style.access", "true") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
    
    # Configuration du master (seulement pour local et simulation)
    if emr_sim.config["spark_master"]:
        builder = builder.master(emr_sim.config["spark_master"])
    
    # üîß APPLICATION CONFIG UNIFI√âE (une seule fois, pas de conflit)
    for key, value in final_config.items():
        builder = builder.config(key, value)
    
    spark = builder.getOrCreate()
    
    # Configuration post-cr√©ation
    spark.sparkContext.setLogLevel("WARN")
    
    # Affichage config appliqu√©e
    memory_config = emr_sim.get_memory_config()
    print(f"‚úÖ Session Spark cr√©√©e - Mode: {mode} - Version: {spark.version}")
    print(f"üìä Config m√©moire: {memory_config['spark.executor.memory']} executor / {memory_config['spark.driver.memory']} driver")
    print(f"üéØ Cores disponibles: {spark.sparkContext.defaultParallelism}")
    
    return spark

def get_s3_client(mode="local"):
    """
    Cr√©e un client S3 adapt√© au mode d'ex√©cution
    
    Args:
        mode (str): "local", "simulation" ou "production"
    
    Returns:
        boto3.client: Client S3 configur√© (None pour mode local)
    """
    if mode == "local":
        print("üí° Mode local : pas de client S3 n√©cessaire")
        return None
    
    emr_sim = EMRSimulation(mode=mode)
    s3_config = emr_sim.get_s3_config()
    
    if s3_config:
        s3_client = boto3.client('s3', **s3_config)
        print(f"‚úÖ Client S3 cr√©√© - Mode: {mode}")
        return s3_client
    else:
        raise ValueError(f"Configuration S3 impossible pour le mode: {mode}")

def create_bucket_if_not_exists(mode="simulation", bucket_name="fruits-p11-simulation"):
    """
    Cr√©e un bucket S3 s'il n'existe pas d√©j√†
    
    Args:
        mode (str): Mode d'ex√©cution
        bucket_name (str): Nom du bucket √† cr√©er
    
    Returns:
        bool: True si bucket cr√©√© ou existe d√©j√†
    """
    if mode == "local":
        print("üí° Mode local : pas de bucket S3 n√©cessaire")
        return True
    
    try:
        s3_client = get_s3_client(mode=mode)
        
        # V√©rification si le bucket existe
        try:
            s3_client.head_bucket(Bucket=bucket_name)
            print(f"‚úÖ Bucket {bucket_name} existe d√©j√†")
            return True
        except:
            # Le bucket n'existe pas, on le cr√©e
            if mode == "simulation":
                s3_client.create_bucket(Bucket=bucket_name)
            else:  # production
                s3_client.create_bucket(
                    Bucket=bucket_name,
                    CreateBucketConfiguration={'LocationConstraint': 'eu-west-1'}
                )
            print(f"‚úÖ Bucket {bucket_name} cr√©√© avec succ√®s")
            return True
            
    except Exception as e:
        print(f"‚ùå Erreur cr√©ation bucket {bucket_name}: {e}")
        return False

def get_optimal_config_for_hardware():
    """
    D√©tecte automatiquement la config optimale selon le hardware disponible
    """
    config_info = {
        "gpu_available": False,
        "recommended_mode": "local",
        "memory_recommendation": "4g/2g"
    }
    
    # D√©tection GPU
    try:
        import tensorflow as tf
        gpus = tf.config.list_physical_devices('GPU')
        if len(gpus) > 0:
            config_info["gpu_available"] = True
            config_info["memory_recommendation"] = "6g/4g"
            print(f"üöÄ GPU d√©tect√©: {gpus[0].name}")
    except:
        print("üíª Mode CPU d√©tect√©")
    
    # D√©tection RAM disponible
    try:
        import psutil
        ram_gb = psutil.virtual_memory().total / (1024**3)
        if ram_gb >= 16:
            config_info["memory_recommendation"] = "6g/4g" if config_info["gpu_available"] else "4g/2g"
        elif ram_gb >= 8:
            config_info["memory_recommendation"] = "4g/2g"
        else:
            config_info["memory_recommendation"] = "2g/1g"
        print(f"üíæ RAM d√©tect√©e: {ram_gb:.1f} GB")
    except:
        print("üíæ RAM non d√©tectable")
    
    return config_info

# Tests rapides si le module est ex√©cut√© directement
if __name__ == "__main__":
    print("üß™ Test du module emr_simulation.py - Version optimis√©e")
    
    # Test d√©tection hardware
    print("\n--- D√©tection Hardware ---")
    hw_config = get_optimal_config_for_hardware()
    print(f"Config recommand√©e: {hw_config}")
    
    # Test des diff√©rents modes
    for mode in ["local", "simulation", "production"]:
        try:
            print(f"\n--- Test mode {mode} ---")
            enable_gpu = hw_config["gpu_available"] and mode == "local"
            emr = EMRSimulation(mode=mode, enable_gpu=enable_gpu)
            print(f"Storage path: {emr.get_storage_path('test')}")
            print(f"Memory config: {emr.get_memory_config()}")
            print(f"S3 config: {emr.get_s3_config()}")
            
            # Test connexion S3 si applicable
            if mode != "local":
                s3_ok = emr.test_s3_connection()
                print(f"Test S3: {'‚úÖ' if s3_ok else '‚ùå'}")
                
        except Exception as e:
            print(f"‚ùå Erreur mode {mode}: {e}")
    
    print("\n‚úÖ Tests termin√©s")