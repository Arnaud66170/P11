#!/bin/bash
# upload_projet_p11_s3.sh
# Upload de ton projet complet vers S3
# Ã€ exÃ©cuter depuis ~/P11/2-python/

echo "ðŸš€ Upload Projet P11 Complet vers S3"
echo "===================================="

# Configuration
BUCKET_NAME="fruits-p11-production"
REGION="eu-west-1"

# VÃ©rification de la localisation
if [ ! -d "data/fruits-360" ]; then
    echo "âŒ Erreur : exÃ©cute ce script depuis ~/P11/2-python/"
    exit 1
fi

echo "ðŸ“‚ Projet dÃ©tectÃ© : $(pwd)"

# 1. DATASET - Upload des donnÃ©es fruits-360
echo "ðŸ“¤ 1/6 Upload du dataset fruits-360..."
aws s3 sync data/fruits-360/ s3://$BUCKET_NAME/data/fruits-360/ \
    --region $REGION \
    --exclude "*.DS_Store" \
    --exclude "Thumbs.db" \
    --delete \
    --quiet

echo "   âœ… Dataset uploadÃ©"

# 2. SCRIPTS - Code Python et notebooks
echo "ðŸ“¤ 2/6 Upload des scripts et code..."

# Scripts EMR et utilitaires
aws s3 sync scripts/ s3://$BUCKET_NAME/scripts/ \
    --region $REGION \
    --exclude "*.log" \
    --quiet

# Code source Python
aws s3 sync src/ s3://$BUCKET_NAME/src/ \
    --region $REGION \
    --exclude "__pycache__/*" \
    --exclude "*.pyc" \
    --exclude ".ipynb_checkpoints/*" \
    --quiet

# Notebooks (sÃ©lection des principaux)
aws s3 cp notebooks/01_fruits_pipeline_cloud.ipynb \
    s3://$BUCKET_NAME/notebooks/01_fruits_pipeline_cloud.ipynb \
    --region $REGION

aws s3 cp notebooks/04_pipeline_fruits_demo.ipynb \
    s3://$BUCKET_NAME/notebooks/04_pipeline_fruits_demo.ipynb \
    --region $REGION

echo "   âœ… Code uploadÃ©"

# 3. RÃ‰SULTATS - Outputs distribuÃ©s (PREUVES!)
echo "ðŸ“¤ 3/6 Upload des rÃ©sultats distribuÃ©s..."

# RÃ©sultats PCA optimaux (PREUVE de calcul distribuÃ©)
aws s3 sync outputs/features_pca_optimal.parquet/ \
    s3://$BUCKET_NAME/outputs/features_pca_optimal.parquet/ \
    --region $REGION \
    --quiet

# RÃ©sultats finaux
aws s3 sync outputs/final_results.parquet/ \
    s3://$BUCKET_NAME/outputs/final_results.parquet/ \
    --region $REGION \
    --quiet

# Cache des features (utile pour la dÃ©mo)
aws s3 sync outputs/cache/ s3://$BUCKET_NAME/outputs/cache/ \
    --region $REGION \
    --quiet

echo "   âœ… RÃ©sultats distribuÃ©s uploadÃ©s"

# 4. VISUALISATIONS - Graphiques pour la soutenance
echo "ðŸ“¤ 4/6 Upload des visualisations..."
aws s3 cp outputs/pca_variance_analysis.png \
    s3://$BUCKET_NAME/outputs/visualizations/pca_variance_analysis.png \
    --region $REGION

aws s3 cp outputs/pca_2d_visualisation.png \
    s3://$BUCKET_NAME/outputs/visualizations/pca_2d_visualisation.png \
    --region $REGION

echo "   âœ… Visualisations uploadÃ©es"

# 5. CONFIGURATION - AWS et Docker
echo "ðŸ“¤ 5/6 Upload des configurations..."
aws s3 sync aws-config/ s3://$BUCKET_NAME/config/aws/ \
    --region $REGION \
    --quiet

# Requirements pour reproduction environnement
aws s3 cp requirements.txt s3://$BUCKET_NAME/config/requirements.txt \
    --region $REGION

echo "   âœ… Configurations uploadÃ©es"

# 6. DOCUMENTATION - README et mÃ©tadonnÃ©es
echo "ðŸ“¤ 6/6 Ajout de la documentation..."

# CrÃ©ation d'un README projet
cat > /tmp/README_P11.md << 'EOF'
# Projet P11 - Pipeline Big Data Fruits
## Architecture Cloud AWS EMR + S3

### Structure du Projet
- **data/fruits-360/** : Dataset images (120 classes, 82k+ images)
- **src/** : Modules Python (preprocessing, PCA, utils)
- **notebooks/** : Pipeline principal et dÃ©mos
- **outputs/** : RÃ©sultats distribuÃ©s (PCA, features)
- **scripts/** : Configuration EMR et bootstrap
- **config/** : Configuration AWS et requirements

### RÃ©sultats ClÃ©s
- âœ… Extraction features MobileNetV2 distribuÃ©e
- âœ… RÃ©duction dimension PCA optimisÃ©e (56 composantes)
- âœ… Calcul distribuÃ© sur EMR validÃ©
- âœ… ConformitÃ© RGPD (rÃ©gion eu-west-1)

### Preuves Techniques
- Fichiers `part-*` : Distribution sur 16 partitions
- Fichier `_SUCCESS` : Validation Spark
- MÃ©tadonnÃ©es parquet : Optimisation stockage

Date de gÃ©nÃ©ration : $(date)
EOF

aws s3 cp /tmp/README_P11.md s3://$BUCKET_NAME/README.md --region $REGION

echo "   âœ… Documentation ajoutÃ©e"

# 7. VÃ‰RIFICATION FINALE
echo ""
echo "ðŸ” VÃ‰RIFICATION S3..."
echo "==================="

# Statistiques par dossier
echo "ðŸ“Š RÃ©partition des objets :"
aws s3 ls s3://$BUCKET_NAME/ --recursive --human-readable --summarize | tail -2

echo ""
echo "ðŸ“ Structure crÃ©Ã©e :"
aws s3 ls s3://$BUCKET_NAME/ --human-readable

echo ""
echo "ðŸŽ¯ PREUVES POUR SOUTENANCE :"
echo "  âœ… Dataset complet sur S3"
echo "  âœ… Code source et notebooks"  
echo "  âœ… RÃ©sultats distribuÃ©s (part-*, _SUCCESS)"
echo "  âœ… Visualisations PCA"
echo "  âœ… Configuration EMR prÃªte"
echo "  âœ… RÃ©gion EU (RGPD compliant)"

echo ""
echo "ðŸŽ‰ UPLOAD TERMINÃ‰ ! Projet prÃªt pour la soutenance."
echo "ðŸ‘€ VÃ©rifier dans la console AWS S3 : https://s3.console.aws.amazon.com/s3/buckets/fruits-p11-production"

# Nettoyage
rm -f /tmp/README_P11.md

# Rendre le script exÃ©cutable :
# chmod +x upload_projet_p11_s3.sh

# ExÃ©cution depuis la racine projet :
# ./upload_projet_p11_s3.sh
