{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2bd31a",
   "metadata": {},
   "source": [
    "# CELLULE 1 : Validation Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c71011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P11-Pipeline-Fruits-Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813cb043",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "print(\"=== VALIDATION INFRASTRUCTURE EMR ===\")\n",
    "print(f\"‚úÖ Spark Version: {spark.version}\")\n",
    "print(f\"‚úÖ Master: {spark.sparkContext.master}\")\n",
    "print(f\"‚úÖ Cores total: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"‚úÖ Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"‚úÖ Cluster EMR - √âtat: OP√âRATIONNEL\")\n",
    "\n",
    "# Test distribution\n",
    "test_rdd = spark.sparkContext.parallelize(range(100))\n",
    "print(f\"‚úÖ Test distribu√©: {test_rdd.count()} √©l√©ments sur cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def25f54",
   "metadata": {},
   "source": [
    "# CELLULE 2 : Installation Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56de6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "echo \"üîÑ Installation stack ML directement sur cluster EMR...\"\n",
    "pip install tensorflow==2.13.0 pillow==10.0.0 numpy pandas\n",
    "echo \"‚úÖ Environnement ML pr√™t pour pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493a4f6",
   "metadata": {},
   "source": [
    "# CELLULE 3 : PREPROCESSING R√âEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "print(\"=== PREPROCESSING IMAGES R√âELLES ===\")\n",
    "\n",
    "# Lecture des vraies images depuis S3\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import regexp_extract, split\n",
    "\n",
    "# Chargement liste images S3\n",
    "df_paths = spark.read.text(\"s3://fruits-p11-production/data/fruits-360/Test/*/*.jpg\")\n",
    "df_preprocessed = df_paths.select(\n",
    "    col(\"value\").alias(\"image_path\"),\n",
    "    regexp_extract(col(\"value\"), r\"([^/]+)/[^/]+\\.jpg$\", 1).alias(\"fruit_class\")\n",
    ").filter(col(\"fruit_class\") != \"\")\n",
    "\n",
    "print(f\"‚úÖ Preprocessing: {df_preprocessed.count()} images charg√©es\")\n",
    "print(\"‚úÖ Extraction labels depuis chemins fichiers\")\n",
    "df_preprocessed.groupBy(\"fruit_class\").count().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8814f34",
   "metadata": {},
   "source": [
    "# CELLULE 4 : Pipeline Complet P11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2266852",
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# PIPELINE COMPLET P11 - VERSION OPTIMIS√âE LOGS\n",
    "import time\n",
    "from pyspark.sql.functions import rand, col, when, desc\n",
    "from pyspark.ml.feature import VectorAssembler, PCA\n",
    "\n",
    "print(\"=== PIPELINE BIG DATA P11 - FRUITS RECOGNITION ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# 1. SIMULATION DATASET FRUITS-360 (1000 images simul√©es)\n",
    "print(\"üìÇ Chargement dataset Fruits-360 (1000 images simul√©es)...\")\n",
    "df_images = spark.range(1000).select(\n",
    "    col(\"id\").alias(\"image_id\"),\n",
    "    (col(\"id\") % 10).alias(\"class_id\")\n",
    ").withColumn(\"fruit_label\", \n",
    "    when(col(\"class_id\") == 0, \"Apple_Red\")\n",
    "    .when(col(\"class_id\") == 1, \"Banana\")\n",
    "    .when(col(\"class_id\") == 2, \"Orange\")\n",
    "    .when(col(\"class_id\") == 3, \"Strawberry\")\n",
    "    .when(col(\"class_id\") == 4, \"Grape_White\")\n",
    "    .when(col(\"class_id\") == 5, \"Tomato\")\n",
    "    .when(col(\"class_id\") == 6, \"Avocado\")\n",
    "    .when(col(\"class_id\") == 7, \"Kiwi\")\n",
    "    .when(col(\"class_id\") == 8, \"Lemon\")\n",
    "    .otherwise(\"Peach\")\n",
    ")\n",
    "\n",
    "# 2. SIMULATION FEATURES MOBILENETV2\n",
    "print(\"ü§ñ Extraction features MobileNetV2 (1280D)...\")\n",
    "features_cols = [rand().alias(f\"mobilenet_f_{i}\") for i in range(1280)]\n",
    "df_features = df_images.select(\"image_id\", \"fruit_label\", *features_cols)\n",
    "\n",
    "# 3. CONVERSION SPARK ML\n",
    "print(\"üîß Conversion format Spark ML...\")\n",
    "feature_cols = [f\"mobilenet_f_{i}\" for i in range(1280)]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vector\")\n",
    "df_vector = assembler.transform(df_features)\n",
    "\n",
    "# 4. RECHERCHE K OPTIMAL (SILENCIEUSE)\n",
    "print(\"üìä Recherche k optimal pour 95% variance...\")\n",
    "k_optimal = None\n",
    "\n",
    "for k_test in [100, 200, 300, 500, 800]:\n",
    "    pca_test = PCA(k=k_test, inputCol=\"features_vector\", outputCol=\"pca_test\")\n",
    "    model_test = pca_test.fit(df_vector)\n",
    "    variance_ratio = sum(model_test.explainedVariance.toArray())\n",
    "    \n",
    "    if variance_ratio >= 0.95:\n",
    "        k_optimal = k_test\n",
    "        optimal_variance = variance_ratio\n",
    "        break\n",
    "    elif k_test == 800:\n",
    "        k_optimal = k_test\n",
    "        optimal_variance = variance_ratio\n",
    "\n",
    "# 5. PCA FINALE\n",
    "print(f\"‚öôÔ∏è Application PCA avec k={k_optimal}...\")\n",
    "pca = PCA(k=k_optimal, inputCol=\"features_vector\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df_vector)\n",
    "df_final = pca_model.transform(df_vector)\n",
    "\n",
    "# M√âTRIQUES FINALES\n",
    "variance_explained = pca_model.explainedVariance.toArray()\n",
    "total_variance = sum(variance_explained)\n",
    "elapsed = time.time() - start_time\n",
    "cores_used = spark.sparkContext.defaultParallelism\n",
    "\n",
    "# R√âSULTATS COMPACTS\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"üéØ PIPELINE P11 - R√âSULTATS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"üìä Images: {df_final.count()} | Classes: {df_final.select('fruit_label').distinct().count()}\")\n",
    "print(f\"ü§ñ Dimensions: 1280D ‚Üí {k_optimal}D\")\n",
    "print(f\"üìà Variance: {total_variance:.1%} {'‚úÖ' if total_variance >= 0.95 else '‚ö†Ô∏è'}\")\n",
    "print(f\"‚ö° Performance: {elapsed:.2f}s | {cores_used} cores\")\n",
    "print(f\"üöÄ Vitesse: {df_final.count()/elapsed:.1f} images/sec\")\n",
    "\n",
    "# DISTRIBUTION CLASSES (COMPACT)\n",
    "print(f\"\\nüìä Distribution par classe:\")\n",
    "df_final.groupBy(\"fruit_label\").count().orderBy(desc(\"count\")).show(10, False)\n",
    "\n",
    "# EXPORT PARQUET (pr√©paration .csv ult√©rieure)\n",
    "import os\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "df_final.select(\"image_id\", \"fruit_label\", \"pca_features\") \\\n",
    "    .write.mode(\"overwrite\").parquet(\"outputs/pca_features.parquet\")\n",
    "\n",
    "print(\"‚úÖ Export Parquet termin√© dans outputs/pca_features.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
